# -*- coding: utf-8 -*-
"""LWC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-EkaLhv1IE3sDYV7Hoich1tjBaAhoOgi

# BERT + Latent World Consistency (LWC) Model Training

This notebook implements the advanced fake news detection model combining:
- **BERT** for article-level encoding
- **Plausibility Model** (DistilBERT) for sentence-level plausibility scoring
- **Consistency Score C(A)** computed from plausibility violations
- **Feature Fusion** combining BERT embeddings with consistency features

## Features
- CUDA acceleration for faster training
- Optimized data loading with pin_memory
- Progress bars and timing information
- Comprehensive evaluation metrics
"""

# Install required packages
!pip install torch transformers pandas scikit-learn nltk tqdm numpy -q

# Download NLTK data
import nltk
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    try:
        nltk.download('punkt_tab', quiet=True)
    except:
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt', quiet=True)

print("✓ Packages installed and NLTK data downloaded")

# Check CUDA availability
import torch

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"✓ CUDA available!")
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  CUDA Version: {torch.version.cuda}")
    print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    device = torch.device("cpu")
    print("⚠ CUDA not available. Using CPU (training will be slower)")

print(f"\nUsing device: {device}")

"""## Step 1: Upload Data Files

Upload `True.csv` and `Fake.csv` files using the file uploader below.

"""

# Upload data files (run this cell and upload True.csv and Fake.csv)
from google.colab import files
import os

# Create data directory
os.makedirs('data', exist_ok=True)

# Upload files
print("Please upload True.csv and Fake.csv files:")
uploaded = files.upload()

# Move files to data directory
for filename in uploaded.keys():
    if filename.endswith('.csv'):
        os.rename(filename, f'data/{filename}')
        print(f"✓ Moved {filename} to data/{filename}")

print("\n✓ Data files ready!")



import os

# Create data directory if it doesn't exist
os.makedirs('data', exist_ok=True)

# Move True.csv to data directory
if os.path.exists('True.csv'):
    os.rename('True.csv', 'data/True.csv')
    print("✓ Moved True.csv to data/True.csv")
else:
    print("⚠ True.csv not found in the root directory. Please ensure it's uploaded.")

from google.colab import drive
drive.mount('/content/drive')

"""## Step 2: Upload Plausibility Model

Upload the `plausability_model_final` folder containing the pre-trained DistilBERT model.

"""

# Upload plausibility model (if you have it as a zip file, uncomment and use)
from google.colab import files
import zipfile

print("Upload plausability_model_final.zip:")
uploaded = files.upload()

# Extract if zip file
for filename in uploaded.keys():
    if filename.endswith('.zip'):
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall('.')
        print(f"✓ Extracted {filename}")
        os.remove(filename)

# OR: If you have the model in Google Drive, mount it:
# from google.colab import drive
# drive.mount('/content/drive')
# # Then copy from drive if needed

print("Note: Ensure plausability_model_final folder is in the current directory")
print("It should contain: config.json, pytorch_model.bin, tokenizer files")

"""## Step 3: Import Libraries and Define Helper Classes

"""

import os
import time
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import AutoModel, BertTokenizerFast, DistilBertForSequenceClassification, DistilBertTokenizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score
from tqdm import tqdm
from typing import List
import nltk

print("✓ All libraries imported")

# ============================================================================
# SENTENCE PROCESSOR
# ============================================================================

def split_into_sentences(text: str) -> List[str]:
    """Split text into sentences using NLTK."""
    if not text or not isinstance(text, str):
        return []
    sentences = nltk.sent_tokenize(text)
    cleaned = [s.strip() for s in sentences if s.strip()]
    return cleaned

def filter_sentences(sentences: List[str], min_tokens: int = 5) -> List[str]:
    """Remove sentences with fewer than min_tokens tokens."""
    filtered = []
    for sent in sentences:
        tokens = sent.split()
        if len(tokens) >= min_tokens:
            filtered.append(sent)
    return filtered

def process_article(
    text: str,
    max_sentences: int = 12,
    min_tokens_per_sentence: int = 5
) -> tuple[List[str], int]:
    """Process article: split into sentences, filter, and truncate/pad."""
    sentences = split_into_sentences(text)
    sentences = filter_sentences(sentences, min_tokens=min_tokens_per_sentence)

    if len(sentences) > max_sentences:
        sentences = sentences[:max_sentences]

    num_sentences = len(sentences)

    if num_sentences < max_sentences:
        sentences.extend([""] * (max_sentences - num_sentences))

    return sentences, num_sentences

def process_articles_batch(
    texts: List[str],
    max_sentences: int = 12,
    min_tokens_per_sentence: int = 5
) -> tuple[List[List[str]], List[int]]:
    """Process a batch of articles."""
    processed_sentences = []
    sentence_counts = []

    for text in texts:
        sentences, count = process_article(
            text,
            max_sentences=max_sentences,
            min_tokens_per_sentence=min_tokens_per_sentence
        )
        processed_sentences.append(sentences)
        sentence_counts.append(count)

    return processed_sentences, sentence_counts

print("✓ Sentence processor defined")

# ============================================================================
# PLAUSIBILITY SCORER
# ============================================================================

class PlausibilityWorldModel:
    """Wrapper for trained DistilBERT plausibility model."""

    def __init__(self, model_path='plausability_model_final', device=None):
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.device = device
        self.model_path = model_path

        if not os.path.exists(model_path):
            raise FileNotFoundError(
                f"Plausibility model not found at {model_path}."
            )

        self.tokenizer = DistilBertTokenizer.from_pretrained(model_path)
        self.model = DistilBertForSequenceClassification.from_pretrained(model_path)
        self.model.to(device)
        self.model.eval()
        self._cache = {}

    def score_claim(self, sentence: str) -> float:
        """Score a single sentence for plausibility."""
        if sentence in self._cache:
            return self._cache[sentence]

        inputs = self.tokenizer(
            sentence,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            score = outputs.logits.squeeze().item()

        score = max(0.0, min(1.0, score))
        self._cache[sentence] = score
        return score

    def score_batch(self, sentences: List[str]) -> List[float]:
        """Score multiple sentences in batch for efficiency."""
        if not sentences:
            return []

        uncached_sentences = []
        uncached_indices = []
        scores = [None] * len(sentences)

        for i, sent in enumerate(sentences):
            if sent in self._cache:
                scores[i] = self._cache[sent]
            else:
                uncached_sentences.append(sent)
                uncached_indices.append(i)

        if uncached_sentences:
            inputs = self.tokenizer(
                uncached_sentences,
                max_length=512,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                batch_scores = outputs.logits.squeeze()

                if batch_scores.dim() == 0:
                    batch_scores = batch_scores.unsqueeze(0)

                batch_scores = batch_scores.cpu().numpy()

            for idx, score in zip(uncached_indices, batch_scores):
                score = max(0.0, min(1.0, float(score)))
                scores[idx] = score
                self._cache[sentences[idx]] = score

        return scores

print("✓ Plausibility scorer defined")

# ============================================================================
# ADVANCED MODEL (BERT + LWC)
# ============================================================================

class AdvancedFakeNewsModel(nn.Module):
    """
    Advanced model that uses:
    1. Sentence-level plausibility scoring
    2. Consistency score C(A) from plausibility violations
    3. Full article BERT encoding
    4. Combined classifier using both features
    """

    def __init__(
        self,
        bert_model_name: str = 'bert-base-uncased',
        freeze_bert: bool = True,
        plausibility_model_path: str = 'plausability_model_final'
    ):
        super(AdvancedFakeNewsModel, self).__init__()

        # Article encoder (BERT)
        self.article_bert = AutoModel.from_pretrained(bert_model_name)
        if freeze_bert:
            for param in self.article_bert.parameters():
                param.requires_grad = False

        # Plausibility scorer (frozen, used for inference)
        self.plausibility_model = PlausibilityWorldModel(
            model_path=plausibility_model_path,
            device=None
        )

        # Combined classifier
        # Input: article embedding (768) + C(A) (1) + mean_plausibility (1) + max_violation (1) = 771
        self.classifier = nn.Sequential(
            nn.Linear(771, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 2)
        )

    def encode_article(
        self,
        article_input_ids: torch.Tensor,
        article_attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """Encode full article using BERT."""
        with torch.no_grad():
            outputs = self.article_bert(
                input_ids=article_input_ids,
                attention_mask=article_attention_mask # Corrected typo here
            )

        article_embedding = outputs.pooler_output
        article_embedding = article_embedding.requires_grad_(True)
        return article_embedding

    def compute_consistency_score(
        self,
        plausibility_scores: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Compute consistency score C(A) from plausibility scores.
        Returns: (C(A), mean_plausibility, max_violation)
        """
        violations = 1.0 - plausibility_scores
        consistency_scores = (violations ** 2).mean(dim=1)
        mean_plausibility = plausibility_scores.mean(dim=1)
        max_violation = violations.max(dim=1)[0]

        return consistency_scores, mean_plausibility, max_violation

    def forward(
        self,
        sentence_texts: list[list[str]],
        article_input_ids: torch.Tensor,
        article_attention_mask: torch.Tensor,
        return_inconsistency: bool = False
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Forward pass."""
        # Use the batch size from the actual input tensors for consistency
        batch_size = article_input_ids.shape[0]

        # 1. Score sentences for plausibility
        all_plausibility_scores = []
        for article_sentences in sentence_texts:
            article_scores = self.plausibility_model.score_batch(article_sentences)
            all_plausibility_scores.append(article_scores)

        # Convert to tensor
        # Ensure max_sentences is correctly derived or fixed if it can vary within a batch
        # Given data processing, it should be config['data_processing']['max_sentences'] (12)
        # If actual max_sentences varies, pad here. For now, assume uniform from pre-processing.
        max_sentences = max(len(scores) for scores in all_plausibility_scores) if all_plausibility_scores else 0
        if max_sentences == 0: # Handle cases where all articles might have no valid sentences after filtering
            # This path assumes that the pre-processor will always return some sentences or padded empty strings
            # so max_sentences should not be 0 under normal operation.
            # If it does become 0, this might indicate an upstream issue in data processing.
            # For now, setting it to a default if unexpectedly 0.
            max_sentences = 1 # Minimal valid size to avoid errors in torch.zeros

        plausibility_tensor = torch.zeros(batch_size, max_sentences, device=article_input_ids.device)
        for i, scores in enumerate(all_plausibility_scores):
            if scores: # Only assign if there are scores to prevent errors with empty lists
                plausibility_tensor[i, :len(scores)] = torch.tensor(
                    scores, device=article_input_ids.device
                )

        # 2. Compute consistency score C(A)
        consistency_scores, mean_plausibility, max_violation = self.compute_consistency_score(
            plausibility_tensor
        )

        # 3. Encode full article
        article_embedding = self.encode_article(
            article_input_ids,
            article_attention_mask
        )

        # 4. Combine features
        combined_features = torch.cat(
            [
                article_embedding,
                consistency_scores.unsqueeze(1),
                mean_plausibility.unsqueeze(1),
                max_violation.unsqueeze(1)
            ],
            dim=1
        )

        # 5. Classify
        logits = self.classifier(combined_features)

        if return_inconsistency:
            return logits, consistency_scores
        return logits, None

    def to(self, device):
        """Move model to device and update plausibility model device."""
        super().to(device)
        if hasattr(self, 'plausibility_model'):
            self.plausibility_model.device = device
            self.plausibility_model.model.to(device)
        return self

print("✓ Advanced model defined")

# ============================================================================
# DATA LOADER
# ============================================================================

def load_data(true_data_path='data/True.csv', fake_data_path='data/Fake.csv'):
    """Load and combine true and fake news datasets."""
    true_data = pd.read_csv(true_data_path)
    fake_data = pd.read_csv(fake_data_path)

    true_data['Target'] = ['True'] * len(true_data)
    fake_data['Target'] = ['Fake'] * len(fake_data)

    label_map = {"Fake": 1, "True": 0}
    true_data['label'] = true_data['Target'].map(label_map)
    fake_data['label'] = fake_data['Target'].map(label_map)

    data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)
    return data

def prepare_data(data, text_column='text', test_size=0.3, val_size=0.5, random_state=2018):
    """Split data into train, validation, and test sets."""
    train_text, temp_text, train_labels, temp_labels = train_test_split(
        data[text_column], data['label'],
        random_state=random_state,
        test_size=test_size,
        stratify=data['Target']
    )

    val_text, test_text, val_labels, test_labels = train_test_split(
        temp_text, temp_labels,
        random_state=random_state,
        test_size=val_size,
        stratify=temp_labels
    )

    return train_text, val_text, test_text, train_labels, val_labels, test_labels

print("✓ Data loader functions defined")

# ============================================================================
# DATA PROCESSOR
# ============================================================================

class AdvancedDataProcessor:
    """Processes data for advanced model with sentence-level tokenization."""

    def __init__(
        self,
        tokenizer_name='bert-base-uncased',
        max_sentences=12,
        max_tokens_per_sentence=48,
        max_tokens_per_article=256,
        min_tokens_per_sentence=5
    ):
        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)
        self.max_sentences = max_sentences
        self.max_tokens_per_sentence = max_tokens_per_sentence
        self.max_tokens_per_article = max_tokens_per_article
        self.min_tokens_per_sentence = min_tokens_per_sentence

    def process_texts(
        self,
        texts: List[str]
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[List[str]], List[int]]:
        """Process texts into sentence-level and article-level tokens."""
        sentence_lists, sentence_counts = process_articles_batch(
            texts,
            max_sentences=self.max_sentences,
            min_tokens_per_sentence=self.min_tokens_per_sentence
        )

        batch_size = len(texts)

        # Tokenize sentences
        sentence_input_ids = []
        sentence_attention_mask = []

        for sentences in sentence_lists:
            sentence_tokens = self.tokenizer.batch_encode_plus(
                sentences,
                max_length=self.max_tokens_per_sentence,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            sentence_input_ids.append(sentence_tokens['input_ids'])
            sentence_attention_mask.append(sentence_tokens['attention_mask'])

        sentence_input_ids = torch.stack(sentence_input_ids)
        sentence_attention_mask = torch.stack(sentence_attention_mask)

        # Tokenize full articles
        article_tokens = self.tokenizer.batch_encode_plus(
            texts,
            max_length=self.max_tokens_per_article,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        article_input_ids = article_tokens['input_ids']
        article_attention_mask = article_tokens['attention_mask']

        return (
            sentence_input_ids,
            sentence_attention_mask,
            article_input_ids,
            article_attention_mask,
            sentence_lists,
            sentence_counts
        )

    def prepare_dataset(
        self,
        texts: List[str],
        labels: List[int]
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[List[str]], torch.Tensor]:
        """Prepare complete dataset."""
        sentence_ids, sentence_mask, article_ids, article_mask, sentence_texts, _ = self.process_texts(texts)
        labels_tensor = torch.tensor(labels, dtype=torch.long)
        return sentence_ids, sentence_mask, article_ids, article_mask, sentence_texts, labels_tensor

print("✓ Data processor defined")

# ============================================================================
# TRAINER
# ============================================================================

class AdvancedTrainer:
    """Trainer for advanced model with plausibility-based consistency."""

    def __init__(
        self,
        model,
        optimizer,
        device='cpu',
        lambda1=0.1,
        lambda2=0.1
    ):
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.lambda1 = lambda1
        self.lambda2 = lambda2
        self.criterion = nn.CrossEntropyLoss()

    def compute_losses(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        inconsistency_scores: torch.Tensor
    ) -> tuple[torch.Tensor, dict]:
        """Compute classification and separation losses."""
        loss_cls = self.criterion(logits, labels)

        # Consistency separation loss
        real_mask = (labels == 0)
        fake_mask = (labels == 1)

        if real_mask.sum() > 0:
            real_consistency = inconsistency_scores[real_mask].mean()
        else:
            real_consistency = torch.tensor(0.0, device=self.device)

        if fake_mask.sum() > 0:
            fake_consistency = inconsistency_scores[fake_mask].mean()
        else:
            fake_consistency = torch.tensor(0.0, device=self.device)

        loss_sep = self.lambda1 * real_consistency - self.lambda2 * fake_consistency
        total_loss = loss_cls + loss_sep

        loss_dict = {
            'classification': loss_cls.item(),
            'separation': loss_sep.item(),
            'total': total_loss.item()
        }

        return total_loss, loss_dict

    def train_epoch(self, train_dataloader, show_progress=False):
        """Train for one epoch."""
        self.model.train()
        total_losses = {
            'classification': 0.0,
            'separation': 0.0,
            'total': 0.0
        }
        num_batches = 0

        if show_progress:
            pbar = tqdm(train_dataloader, desc="Training", unit="batch")
        else:
            pbar = train_dataloader

        for batch in pbar:
            (sentence_ids, sentence_mask, article_ids, article_mask, sentence_texts, labels) = batch

            article_ids = article_ids.to(self.device)
            article_mask = article_mask.to(self.device)
            labels = labels.to(self.device)

            self.optimizer.zero_grad()

            logits, inconsistency_scores = self.model(
                sentence_texts=sentence_texts,
                article_input_ids=article_ids,
                article_attention_mask=article_mask,
                return_inconsistency=True
            )

            loss, loss_dict = self.compute_losses(
                logits, labels, inconsistency_scores
            )

            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()

            for key in total_losses:
                total_losses[key] += loss_dict[key]
            num_batches += 1

            if show_progress:
                pbar.set_postfix({
                    'Loss': f"{loss_dict['total']:.4f}",
                    'Cls': f"{loss_dict['classification']:.4f}",
                    'Sep': f"{loss_dict['separation']:.4f}"
                })

        avg_losses = {key: val / num_batches for key, val in total_losses.items()}
        return avg_losses

print("✓ Trainer defined")

# ============================================================================
# EVALUATOR
# ============================================================================

class AdvancedEvaluator:
    """Evaluator for advanced model with inconsistency score and plausibility analysis."""

    def __init__(self, model, device='cpu'):
        self.model = model
        self.device = device

    def evaluate(self, val_dataloader):
        """Evaluate model on validation set."""
        self.model.eval()

        all_logits = []
        all_labels = []
        all_inconsistency_scores = []
        all_plausibility_scores = []

        with torch.no_grad():
            for batch in val_dataloader:
                (sentence_ids, sentence_mask, article_ids, article_mask, sentence_texts, labels) = batch

                article_ids = article_ids.to(self.device)
                article_mask = article_mask.to(self.device)
                labels = labels.to(self.device)

                logits, inconsistency_scores = self.model(
                    sentence_texts=sentence_texts,
                    article_input_ids=article_ids,
                    article_attention_mask=article_mask,
                    return_inconsistency=True
                )

                batch_plausibility = []
                for article_sentences in sentence_texts:
                    article_plausibility = self.model.plausibility_model.score_batch(article_sentences)
                    batch_plausibility.append(article_plausibility)
                all_plausibility_scores.extend(batch_plausibility)

                all_logits.append(logits.cpu().numpy())
                all_labels.append(labels.cpu().numpy())
                all_inconsistency_scores.append(inconsistency_scores.cpu().numpy())

        all_logits = np.concatenate(all_logits, axis=0)
        all_labels = np.concatenate(all_labels, axis=0)
        all_inconsistency_scores = np.concatenate(all_inconsistency_scores, axis=0)

        predictions = np.argmax(all_logits, axis=1)
        probabilities = torch.softmax(torch.tensor(all_logits), dim=1).numpy()

        accuracy = accuracy_score(all_labels, predictions)
        f1 = f1_score(all_labels, predictions)
        roc_auc = roc_auc_score(all_labels, probabilities[:, 1])

        real_scores = all_inconsistency_scores[all_labels == 0]
        fake_scores = all_inconsistency_scores[all_labels == 1]

        flat_plausibility = [p for article_scores in all_plausibility_scores for p in article_scores]
        if flat_plausibility:
            mean_plausibility = np.mean(flat_plausibility)
            uncertain_count = sum(1 for p in flat_plausibility if p < 0.5)
            uncertain_ratio = uncertain_count / len(flat_plausibility) if flat_plausibility else 0.0
        else:
            mean_plausibility = 0.0
            uncertain_ratio = 0.0

        article_mean_plausibility = [np.mean(scores) if scores else 0.0 for scores in all_plausibility_scores]
        real_plausibility = [article_mean_plausibility[i] for i in range(len(article_mean_plausibility)) if all_labels[i] == 0]
        fake_plausibility = [article_mean_plausibility[i] for i in range(len(article_mean_plausibility)) if all_labels[i] == 1]

        metrics = {
            'accuracy': accuracy,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'mean_inconsistency_real': float(real_scores.mean()) if len(real_scores) > 0 else 0.0,
            'mean_inconsistency_fake': float(fake_scores.mean()) if len(fake_scores) > 0 else 0.0,
            'std_inconsistency_real': float(real_scores.std()) if len(real_scores) > 0 else 0.0,
            'std_inconsistency_fake': float(fake_scores.std()) if len(fake_scores) > 0 else 0.0,
            'mean_plausibility': float(mean_plausibility),
            'uncertain_ratio': float(uncertain_ratio),
            'mean_plausibility_real': float(np.mean(real_plausibility)) if len(real_plausibility) > 0 else 0.0,
            'mean_plausibility_fake': float(np.mean(fake_plausibility)) if len(fake_plausibility) > 0 else 0.0,
        }

        report = classification_report(all_labels, predictions)

        return metrics, report, {
            'predictions': predictions,
            'labels': all_labels,
            'inconsistency_scores': all_inconsistency_scores,
            'probabilities': probabilities,
            'plausibility_scores': all_plausibility_scores
        }

print("✓ Evaluator defined")

"""## Step 4: Load and Prepare Data

"""

# Configuration
config = {
    'data': {
        'true_data_path': 'data/True.csv',
        'fake_data_path': 'data/Fake.csv',
        'text_column': 'text'  # Use full text for advanced model
    },
    'model': {
        'bert_model_name': 'bert-base-uncased',
        'freeze_bert': True,
        'plausibility_model_path': 'plausability_model_final'
    },
    'data_processing': {
        'max_sentences': 12,
        'max_tokens_per_sentence': 48,
        'max_tokens_per_article': 256,
        'min_tokens_per_sentence': 5
    },
    'training': {
        'batch_size': 8,
        'learning_rate': 2e-5,
        'epochs': 3,
        'test_size': 0.3,
        'val_size': 0.5,
        'random_state': 2018,
        'lambda1': 0.1,
        'lambda2': 0.1
    },
    'paths': {
        'model_save_path': 'advanced_model_weights.pt'
    }
}

print("✓ Configuration set")

# Load and prepare data
print("Loading data...")
data = load_data(
    true_data_path=config['data']['true_data_path'],
    fake_data_path=config['data']['fake_data_path']
)
print(f"✓ Data loaded. Shape: {data.shape}")

# Use 'text' column for advanced model (full article text)
text_column = config['data']['text_column']
if text_column not in data.columns:
    print(f"⚠ '{text_column}' column not found. Using 'title' instead.")
    text_column = 'title'

train_text, val_text, test_text, train_labels, val_labels, test_labels = prepare_data(
    data,
    text_column=text_column,
    test_size=config['training']['test_size'],
    val_size=config['training']['val_size'],
    random_state=config['training']['random_state']
)

print(f"✓ Data split:")
print(f"  Train: {len(train_text)} samples")
print(f"  Validation: {len(val_text)} samples")
print(f"  Test: {len(test_text)} samples")

# Initialize data processor
print("\nInitializing data processor...")
processor = AdvancedDataProcessor(
    tokenizer_name=config['model']['bert_model_name'],
    max_sentences=config['data_processing']['max_sentences'],
    max_tokens_per_sentence=config['data_processing']['max_tokens_per_sentence'],
    max_tokens_per_article=config['data_processing']['max_tokens_per_article'],
    min_tokens_per_sentence=config['data_processing']['min_tokens_per_sentence']
)

# Process data
print("Processing training data...")
train_sentence_ids, train_sentence_mask, train_article_ids, train_article_mask, train_sentence_texts, train_labels_tensor = \
    processor.prepare_dataset(train_text.tolist(), train_labels.tolist())

print("Processing validation data...")
val_sentence_ids, val_sentence_mask, val_article_ids, val_article_mask, val_sentence_texts, val_labels_tensor = \
    processor.prepare_dataset(val_text.tolist(), val_labels.tolist())

print("Processing test data...")
test_sentence_ids, test_sentence_mask, test_article_ids, test_article_mask, test_sentence_texts, test_labels_tensor = \
    processor.prepare_dataset(test_text.tolist(), test_labels.tolist())

print("✓ Data processing complete")

# ============================================================================
# DATASET CLASS
# ============================================================================

class AdvancedDataset(Dataset):
    """Dataset class for advanced model."""

    def __init__(self, sentence_ids, sentence_mask, article_ids, article_mask, sentence_texts, labels):
        self.sentence_ids = sentence_ids
        self.sentence_mask = sentence_mask
        self.article_ids = article_ids
        self.article_mask = article_mask
        self.sentence_texts = sentence_texts
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return (
            self.sentence_ids[idx],
            self.sentence_mask[idx],
            self.article_ids[idx],
            self.article_mask[idx],
            self.sentence_texts[idx],
            self.labels[idx]
        )

# Custom collate function to handle the list of strings
def custom_collate_fn(batch_items):
    # batch_items is a list of tuples (sentence_ids_i, sentence_mask_i, ..., labels_i)

    # Separate the items into their respective lists
    s_ids, s_mask, a_ids, a_mask, s_texts_list, labs = zip(*batch_items)

    # Stack the tensors
    s_ids_batch = torch.stack(s_ids)
    s_mask_batch = torch.stack(s_mask)
    a_ids_batch = torch.stack(a_ids)
    a_mask_batch = torch.stack(a_mask)
    labs_batch = torch.stack(labs)

    # For sentence_texts, we just need the list of lists of strings.
    # `s_texts_list` is already a tuple of `List[str]` from `zip(*)`, convert to list.
    s_texts_batch = list(s_texts_list)

    return (s_ids_batch, s_mask_batch, a_ids_batch, a_mask_batch, s_texts_batch, labs_batch)


# Create datasets
train_dataset = AdvancedDataset(
    train_sentence_ids, train_sentence_mask,
    train_article_ids, train_article_mask, train_sentence_texts, train_labels_tensor
)
val_dataset = AdvancedDataset(
    val_sentence_ids, val_sentence_mask,
    val_article_ids, val_article_mask, val_sentence_texts, val_labels_tensor
)
test_dataset = AdvancedDataset(
    test_sentence_ids, test_sentence_mask,
    test_article_ids, test_article_mask, test_sentence_texts, test_labels_tensor
)

# Create data loaders with optimizations
print("\nCreating data loaders...")
train_dataloader = DataLoader(
    train_dataset,
    sampler=RandomSampler(train_dataset),
    batch_size=config['training']['batch_size'],
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False,
    collate_fn=custom_collate_fn # Add the custom collate function
)
val_dataloader = DataLoader(
    val_dataset,
    sampler=SequentialSampler(val_dataset),
    batch_size=config['training']['batch_size'],
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False,
    collate_fn=custom_collate_fn # Add the custom collate function
)
test_dataloader = DataLoader(
    test_dataset,
    sampler=SequentialSampler(test_dataset),
    batch_size=config['training']['batch_size'],
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False,
    collate_fn=custom_collate_fn # Add the custom collate function
)

print("✓ Data loaders created")

"""## Step 5: Create Model and Start Training

"""

# Create model
print("\nCreating advanced model...")

# Ensure 'device' is defined
import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
print(f"Using device: {device}")

model = AdvancedFakeNewsModel(
    bert_model_name=config['model']['bert_model_name'],
    freeze_bert=config['model']['freeze_bert'],
    plausibility_model_path=config['model']['plausibility_model_path']
)
model.to(device)

# Initialize optimizer
optimizer = AdamW(model.parameters(), lr=config['training']['learning_rate'])

# Initialize trainer and evaluator
trainer = AdvancedTrainer(
    model,
    optimizer,
    device=device,
    lambda1=config['training']['lambda1'],
    lambda2=config['training']['lambda2']
)
evaluator = AdvancedEvaluator(model, device=device)

print("✓ Model, trainer, and evaluator initialized")
print(f"  Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable")

# Training loop
print("\n" + "="*70)
print("STARTING TRAINING")
print("="*70)

best_val_accuracy = 0.0

for epoch in range(config['training']['epochs']):
    epoch_start_time = time.time()
    print(f"\n{'='*70}")
    print(f"Epoch {epoch+1}/{config['training']['epochs']}")
    print(f"{'='*70}")

    # Train with progress bar
    print("\nTraining...")
    train_losses = trainer.train_epoch(train_dataloader, show_progress=True)
    train_time = time.time() - epoch_start_time

    print(f"\nTraining Losses:")
    print(f"  Classification: {train_losses['classification']:.4f}")
    print(f"  Separation: {train_losses['separation']:.4f}")
    print(f"  Total: {train_losses['total']:.4f}")
    print(f"  Training Time: {train_time:.2f} seconds ({train_time/60:.2f} minutes)")

    # Evaluate
    print("\nEvaluating...")
    eval_start_time = time.time()
    val_metrics, val_report, _ = evaluator.evaluate(val_dataloader)
    eval_time = time.time() - eval_start_time

    print(f"\nValidation Metrics:")
    print(f"  Accuracy: {val_metrics['accuracy']:.4f}")
    print(f"  F1-Score: {val_metrics['f1_score']:.4f}")
    print(f"  ROC-AUC: {val_metrics['roc_auc']:.4f}")
    print(f"  Mean Inconsistency (Real): {val_metrics['mean_inconsistency_real']:.4f}")
    print(f"  Mean Inconsistency (Fake): {val_metrics['mean_inconsistency_fake']:.4f}")
    print(f"  Mean Plausibility (Real): {val_metrics['mean_plausibility_real']:.4f}")
    print(f"  Mean Plausibility (Fake): {val_metrics['mean_plausibility_fake']:.4f}")
    print(f"  Uncertain Ratio: {val_metrics['uncertain_ratio']:.2%}")
    print(f"  Evaluation Time: {eval_time:.2f} seconds")

    # Save best model
    if val_metrics['accuracy'] > best_val_accuracy:
        best_val_accuracy = val_metrics['accuracy']
        torch.save(model.state_dict(), config['paths']['model_save_path'])
        print(f"\n✓ Model saved with validation accuracy: {best_val_accuracy:.4f}")

    epoch_total_time = time.time() - epoch_start_time
    print(f"\nEpoch {epoch+1} Total Time: {epoch_total_time:.2f} seconds ({epoch_total_time/60:.2f} minutes)")

print(f"\n{'='*70}")
print("TRAINING COMPLETE")
print(f"{'='*70}")

"""## Step 6: Evaluate on Test Set

"""

# Load best model and evaluate on test set
print(f"\n{'='*70}")
print("Evaluating on test set...")
print(f"{'='*70}")

model.load_state_dict(torch.load(config['paths']['model_save_path']))

test_metrics, test_report, test_results = evaluator.evaluate(test_dataloader)

print(f"\nTest Set Metrics:")
print(f"  Accuracy: {test_metrics['accuracy']:.4f}")
print(f"  F1-Score: {test_metrics['f1_score']:.4f}")
print(f"  ROC-AUC: {test_metrics['roc_auc']:.4f}")
print(f"  Mean Inconsistency (Real): {test_metrics['mean_inconsistency_real']:.4f}")
print(f"  Mean Inconsistency (Fake): {test_metrics['mean_inconsistency_fake']:.4f}")
print(f"  Std Inconsistency (Real): {test_metrics['std_inconsistency_real']:.4f}")
print(f"  Std Inconsistency (Fake): {test_metrics['std_inconsistency_fake']:.4f}")
print(f"  Mean Plausibility (Real): {test_metrics['mean_plausibility_real']:.4f}")
print(f"  Mean Plausibility (Fake): {test_metrics['mean_plausibility_fake']:.4f}")
print(f"  Overall Mean Plausibility: {test_metrics['mean_plausibility']:.4f}")
print(f"  Uncertain Ratio: {test_metrics['uncertain_ratio']:.2%}")

print(f"\nClassification Report:")
print(test_report)

print(f"\n✓ Model saved to: {config['paths']['model_save_path']}")

"""## Step 7: Download Model

Download the trained model weights.

"""

# Download model
from google.colab import files

files.download(config['paths']['model_save_path'])
print("✓ Model downloaded!")

"""# Task
Create a single code cell that defines the path to `WELFake.csv`, loads the dataset, inverts the 'Label' column (0=fake, 1=real to 0=real, 1=fake), processes the 'Text' and 'Label' columns using `AdvancedDataProcessor` and the existing `config`, creates an `AdvancedDataset` and `DataLoader` with `custom_collate_fn`, initializes `AdvancedFakeNewsModel`, loads `advanced_model_weights.pt`, moves the model to the appropriate device, initializes `AdvancedEvaluator`, runs the evaluation on the WELFake `DataLoader`, and prints the accuracy, F1-score, ROC-AUC, and classification report on the WELFake dataset.

## Consolidate WELFake Evaluation Code

### Subtask:
Create a single code cell to evaluate the trained model on the WELFake dataset.

**Reasoning**:
I need to create a single code cell to perform the evaluation on the WELFake dataset. This involves loading the dataset, preprocessing it, inverting labels, loading the previously trained model, and then running the evaluation to print metrics and the classification report as per the instructions.
"""

import pandas as pd
import torch

print(f"\n{'='*70}")
print("EVALUATING MODEL ON WELFAKE DATASET")
print(f"{'='*70}")

# 1. Define welfake_data_path
welfake_data_path = 'data/WELFake.csv'
print(f"Loading WELFake dataset from: {welfake_data_path}")

# 2. Load WELFake.csv dataset
try:
    welfake_df = pd.read_csv(welfake_data_path)
    print(f"✓ WELFake dataset loaded successfully. Shape: {welfake_df.shape}")
except FileNotFoundError:
    print(f"Error: {welfake_data_path} not found. Please ensure it is uploaded.")
    exit()

# 3. Invert 'Label' column: 0=fake, 1=real (original) -> 0=real, 1=fake (model expected)
welfake_df['Label'] = welfake_df['label'].apply(lambda x: 1 - x) # Assuming 'label' is the column containing original labels
print("✓ 'Label' column inverted to match model's expectation (0=real, 1=fake).")

# Assuming the text column in WELFake.csv is named 'text'
# If it's 'title' or another name, adjust here.
welfake_text_column = 'text'
if welfake_text_column not in welfake_df.columns:
    print(f"⚠ '{welfake_text_column}' column not found in WELFake. Using 'title' instead if available.")
    welfake_text_column = 'title'
    if welfake_text_column not in welfake_df.columns:
        print(f"Error: Neither 'text' nor 'title' column found in WELFake.csv.")
        exit()

# 4. Initialize AdvancedDataProcessor
print("\nInitializing data processor for WELFake...")
processor = AdvancedDataProcessor(
    tokenizer_name=config['model']['bert_model_name'],
    max_sentences=config['data_processing']['max_sentences'],
    max_tokens_per_sentence=config['data_processing']['max_tokens_per_sentence'],
    max_tokens_per_article=config['data_processing']['max_tokens_per_article'],
    min_tokens_per_sentence=config['data_processing']['min_tokens_per_sentence']
)

# 5. Process WELFake data
print("Processing WELFake data...")
welfake_sentence_ids, welfake_sentence_mask, welfake_article_ids, welfake_article_mask, welfake_sentence_texts, welfake_labels_tensor = \
    processor.prepare_dataset(welfake_df[welfake_text_column].tolist(), welfake_df['Label'].tolist())
print("✓ WELFake data processing complete.")

# 6. Create an AdvancedDataset instance
welfake_dataset = AdvancedDataset(
    welfake_sentence_ids, welfake_sentence_mask,
    welfake_article_ids, welfake_article_mask, welfake_sentence_texts, welfake_labels_tensor
)

# 7. Create a DataLoader for WELFake dataset
print("Creating DataLoader for WELFake...")
welfake_dataloader = DataLoader(
    welfake_dataset,
    sampler=SequentialSampler(welfake_dataset),
    batch_size=config['training']['batch_size'],
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False,
    collate_fn=custom_collate_fn
)
print("✓ WELFake DataLoader created.")

# Ensure 'device' is defined (it should be from previous cells, but for safety)
if 'device' not in locals():
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
print(f"Using device for evaluation: {device}")

# 8. Initialize AdvancedFakeNewsModel
print("\nInitializing model for evaluation...")
model_eval = AdvancedFakeNewsModel(
    bert_model_name=config['model']['bert_model_name'],
    freeze_bert=config['model']['freeze_bert'],
    plausibility_model_path=config['model']['plausibility_model_path']
)

# 9. Load the pre-trained model weights
model_eval.load_state_dict(torch.load(config['paths']['model_save_path'], map_location=device))

# 10. Move the model to the device
model_eval.to(device)
model_eval.eval() # Set model to evaluation mode
print("✓ Model initialized and weights loaded.")

# 11. Initialize AdvancedEvaluator
print("\nInitializing evaluator...")
evaluator_welfake = AdvancedEvaluator(model_eval, device=device)
print("✓ Evaluator initialized.")

# 12. Run the evaluation on the WELFake DataLoader
print("\nRunning evaluation on WELFake dataset...")
welfake_metrics, welfake_report, _ = evaluator_welfake.evaluate(welfake_dataloader)
print("✓ Evaluation complete.")

# 13. Print evaluation metrics
print(f"\n{'='*70}")
print("WELFAKE EVALUATION METRICS:")
print(f"  Accuracy: {welfake_metrics['accuracy']:.4f}")
print(f"  F1-Score: {welfake_metrics['f1_score']:.4f}")
print(f"  ROC-AUC: {welfake_metrics['roc_auc']:.4f}")
print(f"  Mean Inconsistency (Real): {welfake_metrics['mean_inconsistency_real']:.4f}")
print(f"  Mean Inconsistency (Fake): {welfake_metrics['mean_inconsistency_fake']:.4f}")
print(f"  Std Inconsistency (Real): {welfake_metrics['std_inconsistency_real']:.4f}")
print(f"  Std Inconsistency (Fake): {welfake_metrics['std_inconsistency_fake']:.4f}")
print(f"  Mean Plausibility (Real): {welfake_metrics['mean_plausibility_real']:.4f}")
print(f"  Mean Plausibility (Fake): {welfake_metrics['mean_plausibility_fake']:.4f}")
print(f"  Overall Mean Plausibility: {welfake_metrics['mean_plausibility']:.4f}")
print(f"  Uncertain Ratio: {welfake_metrics['uncertain_ratio']:.2%}")
print(f"{'='*70}")

# 14. Print classification report
print("\nClassification Report for WELFake Dataset:")
print(welfake_report)

"""**Reasoning**:
The previous code failed because 'WELFake.csv' was not found, which caused a 'NameError' for 'welfake_df'. I will modify the code to initialize 'welfake_df' to None and add explicit checks using `sys.exit()` to gracefully stop execution if the file is not found or essential columns are missing, preventing subsequent errors.


"""